{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Video herunterladen (optional - bei Verwendung von YouTube)\n",
    "video_url = \"https://www.youtube.com/watch?v=EkrxauaWNGc\"\n",
    "yt = YouTube(video_url)\n",
    "yt.streams.filter(progressive=True, file_extension='mp4').first().download(output_path=\".\", filename=\"input_video.mp4\")\n",
    "\n",
    "# 2. Gesichtserkennung im Video\n",
    "video_path = 'input_video.mp4'\n",
    "target_image = 'target_face.jpg'  # Bild der Zielperson\n",
    "target_encoding = face_recognition.face_encodings(face_recognition.load_image_file(target_image))[0]\n",
    "\n",
    "video = cv2.VideoCapture(video_path)\n",
    "frame_rate = int(video.get(cv2.CAP_PROP_FPS))\n",
    "matching_frames = []\n",
    "\n",
    "# frame_number = 0\n",
    "# while video.isOpened():\n",
    "#     ret, frame = video.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     frame_number += 1\n",
    "#     if frame_number % frame_rate == 0:  # Jeden Sekunde-Frame überprüfen\n",
    "#         rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         face_locations = face_recognition.face_locations(rgb_frame)\n",
    "#         face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "        \n",
    "#         for face_encoding in face_encodings:\n",
    "#             if face_recognition.compare_faces([target_encoding], face_encoding)[0]:\n",
    "#                 matching_frames.append(frame_number / frame_rate)\n",
    "# video.release()\n",
    "\n",
    "# # 3. Audio-Extraktion\n",
    "# ffmpeg.input(video_path).output('audio.wav').run()\n",
    "\n",
    "# # 4. Stimmerkennung und Transkription mit Whisper\n",
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# # Whisper model laden\n",
    "# processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "# # Audio transkribieren\n",
    "# def transcribe_audio(audio_path):\n",
    "#     audio_input = processor(audio_path, return_tensors=\"pt\")\n",
    "#     predicted_ids = model.generate(audio_input[\"input_values\"])\n",
    "#     transcript = processor.decode(predicted_ids[0])\n",
    "#     return transcript\n",
    "\n",
    "# transcript = transcribe_audio(\"audio.wav\")\n",
    "\n",
    "# # 5. Emotionale Analyse der Audioaufnahme (wav2vec2)\n",
    "# from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "# # Wav2Vec2 model laden für emotionale Analyse\n",
    "# wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
    "# wav2vec2_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
    "\n",
    "# def analyze_audio_emotion(audio_path):\n",
    "#     audio_input = wav2vec2_processor(audio_path, return_tensors=\"pt\", sampling_rate=16000)\n",
    "#     audio_emotion = wav2vec2_model(audio_input[\"input_values\"]).logits.argmax(axis=-1).item()\n",
    "#     emotions = [\"angry\", \"happy\", \"sad\", \"neutral\"]  # Emotionen-Kategorien\n",
    "#     return emotions[audio_emotion]\n",
    "\n",
    "# audio_emotion = analyze_audio_emotion(\"audio.wav\")\n",
    "\n",
    "# # 6. Emotionserkennung aus Bild (Gesicht)\n",
    "# emotion_detector = FER(mtcnn=True)\n",
    "# frame_emotions = [emotion_detector.detect_emotions(frame) for frame in matching_frames]\n",
    "\n",
    "# # 7. Emotionserkennung aus Text (Sentiment-Analyse)\n",
    "# sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "# text_emotions = sentiment_pipeline(transcript)\n",
    "\n",
    "# # 8. Ergebnisse speichern\n",
    "# results = {\n",
    "#     \"scenes\": [\n",
    "#         {\"time\": time, \"frame_emotion\": emotion, \"text_emotion\": text_emotions, \"audio_emotion\": audio_emotion}\n",
    "#         for time, emotion in zip(matching_frames, frame_emotions)\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# import json\n",
    "# with open('output.json', 'w') as f:\n",
    "#     json.dump(results, f)\n",
    "\n",
    "# print(\"Emotionserkennung abgeschlossen. Ergebnisse gespeichert in 'output.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frames where Target Face is visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting Frames of Target Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frames where Target Voice is audible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exracting Frames of Target Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca13f25e6c54a7b81d9beb1807aa06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d2f5d457c9423ab22cd443504ed4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32aaa007e4a47caaeb1e0906caa483b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/262 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the `decoder` for Hatman/audio-emotion-detection. Defaulting to raw CTC. Error: No module named 'kenlm'\n",
      "Try to install `kenlm`: `pip install kenlm\n",
      "Try to install `pyctcdecode`: `pip install pyctcdecode\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "WARNING:py.warnings:/tmp/ipykernel_21495/62415081.py:9: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, rate = librosa.load(audio_file, sr=16000)  # Ensure it's resampled to 16kHz\n",
      "\n",
      "WARNING:py.warnings:/home/pia/.local/lib/python3.8/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.5844987034797668, 'label': 'Happy'}, {'score': 0.19212304055690765, 'label': 'Angry'}, {'score': 0.17521733045578003, 'label': 'Disgusted'}, {'score': 0.03364561125636101, 'label': 'Fearful'}, {'score': 0.0062223924323916435, 'label': 'Suprised'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification, AutoProcessor, pipeline\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# pipe = pipeline(\"audio-classification\", model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")     \n",
    "pipe = pipeline(\"audio-classification\", model=\"Hatman/audio-emotion-detection\")     \n",
    "# Load your audio file using librosa\n",
    "audio_file = \"audio.wav\"\n",
    "audio, rate = librosa.load(audio_file, sr=16000)  # Ensure it's resampled to 16kHz\n",
    "\n",
    "# Get the emotion prediction\n",
    "result = pipe(audio)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transkribiere die Audiodatei...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/pia/.local/lib/python3.8/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transkribierter Text:  Okay, sir, you agree with me. You don't actually have to go to Bronze Circle to get Google Maps to generate these images. All you have to do is go to the website.\n",
      "Analysiere die Emotionen im Text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "WARNING:py.warnings:/home/pia/.local/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'anger', 'score': 0.010466489009559155}, {'label': 'disgust', 'score': 0.010573158971965313}, {'label': 'fear', 'score': 0.0021569335367530584}, {'label': 'joy', 'score': 0.0030502486042678356}, {'label': 'neutral', 'score': 0.9656197428703308}, {'label': 'sadness', 'score': 0.0030216046143323183}, {'label': 'surprise', 'score': 0.0051116799004375935}]]\n",
      "Erkannte Emotionen mit Scores:\n",
      "anger: 0.01\n",
      "disgust: 0.01\n",
      "fear: 0.00\n",
      "joy: 0.00\n",
      "neutral: 0.97\n",
      "sadness: 0.00\n",
      "surprise: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Whisper for text transcription\n",
    "# Detecting Feeling in Text\n",
    "\n",
    "import whisper\n",
    "from transformers import pipeline\n",
    "\n",
    "# Schritt 1: Audio transkribieren\n",
    "def transcribe_audio(audio_path):\n",
    "    # Whisper-Modell laden\n",
    "    model = whisper.load_model(\"base\")  # Du kannst auch \"small\", \"medium\", \"large\" wählen\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Schritt 2: Emotionen im Text analysieren\n",
    "def analyze_emotion(text):\n",
    "    # Emotionserkennungsmodell laden\n",
    "    emotion_pipeline = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
    "    emotions = emotion_pipeline(text)\n",
    "    return emotions\n",
    "\n",
    "# Audio-Datei Pfad\n",
    "audio_file = \"audio.wav\"\n",
    "\n",
    "# Transkription\n",
    "print(\"Transkribiere die Audiodatei...\")\n",
    "transcribed_text = transcribe_audio(audio_file)\n",
    "print(f\"Transkribierter Text: {transcribed_text}\")\n",
    "\n",
    "# Emotionserkennung\n",
    "print(\"Analysiere die Emotionen im Text...\")\n",
    "emotions = analyze_emotion(transcribed_text)\n",
    "print(emotions)\n",
    "print(\"Erkannte Emotionen mit Scores:\")\n",
    "for emotion in emotions[0]:  # emotions[0] enthält die Liste der Emotionen\n",
    "    print(f\"{emotion['label']}: {emotion['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image/Video\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "\n",
    "def extract_frames(video_path, frame_rate=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    interval = int(fps / frame_rate)  # Frame interval\n",
    "    frame_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % interval == 0:\n",
    "            frames.append(frame)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def analyze_emotions(frames):\n",
    "    emotions = []\n",
    "    for idx, frame in enumerate(frames):\n",
    "        result = DeepFace.analyze(frame, actions=[\"emotion\"], enforce_detection=False)\n",
    "        emotions.append((idx, result['dominant_emotion']))\n",
    "    return emotions\n",
    "\n",
    "video_file = \"video.mp4\"\n",
    "\n",
    "# Extrahiere Frames\n",
    "print(\"Extrahiere Frames aus dem Video...\")\n",
    "frames = extract_frames(video_file, frame_rate=1)  # 1 Frame pro Sekunde\n",
    "\n",
    "# Analysiere Emotionen\n",
    "print(\"Analysiere Emotionen in den Frames...\")\n",
    "emotions = analyze_emotions(frames)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"Ergebnisse der Emotionserkennung:\")\n",
    "for idx, emotion in emotions:\n",
    "    print(f\"Frame {idx}: {emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "from collections import Counter\n",
    "\n",
    "def detect_emotions_in_video(video_path):\n",
    "    \"\"\"\n",
    "    Detect and count emotions in a video\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input video file\n",
    "    \"\"\"\n",
    "    # Initialize the FER detector\n",
    "    emotion_detector = FER(mtcnn=True)\n",
    "    \n",
    "    # Open the video file\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare to store emotion results\n",
    "    emotion_counts = Counter()\n",
    "    total_detected_frames = 0\n",
    "    \n",
    "    # Frame-by-frame emotion detection\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = video_capture.read()\n",
    "        \n",
    "        # Break the loop if no more frames\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Detect emotions in the frame\n",
    "        detected_emotions = emotion_detector.detect_emotions(frame)\n",
    "        \n",
    "        # Process detected emotions\n",
    "        if detected_emotions:\n",
    "            total_detected_frames += 1\n",
    "            for emotion_result in detected_emotions:\n",
    "                # Get dominant emotion\n",
    "                emotions = emotion_result['emotions']\n",
    "                dominant_emotion = max(emotions, key=emotions.get)\n",
    "                emotion_counts[dominant_emotion] += 1\n",
    "    \n",
    "    # Release resources\n",
    "    video_capture.release()\n",
    "    \n",
    "    # Print emotion occurrence summary\n",
    "    print(\"\\nEmotion Occurrence Summary:\")\n",
    "    for emotion, count in emotion_counts.items():\n",
    "        percentage = (count / total_detected_frames) * 100 if total_detected_frames > 0 else 0\n",
    "        print(f\"{emotion}: {count} times ({percentage:.2f}%)\")\n",
    "    \n",
    "    return dict(emotion_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/pia/.local/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "\n",
      "WARNING:py.warnings:/home/pia/.local/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "\n",
      "WARNING:py.warnings:/home/pia/.local/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion Occurrence Summary:\n",
      "neutral: 414 times (128.17%)\n",
      "happy: 415 times (128.48%)\n",
      "angry: 323 times (100.00%)\n",
      "sad: 296 times (91.64%)\n",
      "fear: 85 times (26.32%)\n",
      "surprise: 2 times (0.62%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neutral': 414,\n",
       " 'happy': 415,\n",
       " 'angry': 323,\n",
       " 'sad': 296,\n",
       " 'fear': 85,\n",
       " 'surprise': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = \"input_video.mp4\"\n",
    "detect_emotions_in_video(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
